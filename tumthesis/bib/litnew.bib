@misc{schlichtkrull2017modelingrelationaldatagraph,
      title={Modeling Relational Data with Graph Convolutional Networks}, 
      author={Michael Schlichtkrull and Thomas N. Kipf and Peter Bloem and Rianne van den Berg and Ivan Titov and Max Welling},
      year={2017},
      eprint={1703.06103},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1703.06103}, 
}

@misc{tomani2022parameterizedtemperaturescalingboosting,
      title={Parameterized Temperature Scaling for Boosting the Expressive Power in Post-Hoc Uncertainty Calibration}, 
      author={Christian Tomani and Daniel Cremers and Florian Buettner},
      year={2022},
      eprint={2102.12182},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2102.12182}, 
}

@misc{gal2016dropoutbayesianapproximationrepresenting,
      title={Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}, 
      author={Yarin Gal and Zoubin Ghahramani},
      year={2016},
      eprint={1506.02142},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1506.02142}, 
}

@misc{lakshminarayanan2017simplescalablepredictiveuncertainty,
      title={Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}, 
      author={Balaji Lakshminarayanan and Alexander Pritzel and Charles Blundell},
      year={2017},
      eprint={1612.01474},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1612.01474}, 
}

@misc{guo2017calibrationmodernneuralnetworks,
      title={On Calibration of Modern Neural Networks}, 
      author={Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
      year={2017},
      eprint={1706.04599},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.04599}, 
}

@article{yang2014embedding,
  title={Embedding entities and relations for learning and inference in knowledge bases},
  author={Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
  journal={arXiv preprint arXiv:1412.6575},
  year={2014}
}

@article{rong2019dropedge,
  title={Dropedge: Towards deep graph convolutional networks on node classification},
  author={Rong, Yu and Huang, Wenbing and Xu, Tingyang and Huang, Junzhou},
  journal={arXiv preprint arXiv:1907.10903},
  year={2019}
}

@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}

@inproceedings{pujara2017sparsity,
  title={Sparsity and noise: Where knowledge graph embeddings fall short},
  author={Pujara, Jay and Augustine, Eriq and Getoor, Lise},
  booktitle={Proceedings of the 2017 conference on empirical methods in natural language processing},
  pages={1751--1756},
  year={2017}
}

@inproceedings{dettmers2018convolutional,
  title={Convolutional 2d knowledge graph embeddings},
  author={Dettmers, Tim and Minervini, Pasquale and Stenetorp, Pontus and Riedel, Sebastian},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{toutanova2015observed,
  title={Observed versus latent features for knowledge base and text inference},
  author={Toutanova, Kristina and Chen, Danqi},
  booktitle={Proceedings of the 3rd workshop on continuous vector space models and their compositionality},
  pages={57--66},
  year={2015}
}


@misc{kipf2017semisupervisedclassificationgraphconvolutional,
      title={Semi-Supervised Classification with Graph Convolutional Networks}, 
      author={Thomas N. Kipf and Max Welling},
      year={2017},
      eprint={1609.02907},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1609.02907}, 
}

@inproceedings{bordes_translating_2013,
	title = {Translating {Embeddings} for {Modeling} {Multi}-relational {Data}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
	editor = {Burges, C. J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{xiao2021node,
  title={Node classification using graph convolutional network with dropout regularization},
  author={Xiao, Bing-Yu and Tseng, Chien-Cheng and Lee, Su-Ling},
  booktitle={TENCON 2021-2021 IEEE Region 10 Conference (TENCON)},
  pages={84--87},
  year={2021},
  organization={IEEE}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={pmlr}
}

@inproceedings{luobeyond,
  title={Beyond random masking: When dropout meets graph convolutional networks},
  author={Luo, Yuankai and Wu, Xiao-Ming and Zhu, Hao},
  booktitle={The Thirteenth International Conference on Learning Representations}
}

@inproceedings{zhu2023closerlookprobabilitycalibration,
author = {Zhu, Ruiqi and Wang, Fangrong and Bundy, Alan and Li, Xue and Nuamah, Kwabena and Xu, Lei and Mauceri, Stefano and Pan, Jeff Z.},
title = {A Closer Look at Probability Calibration of Knowledge Graph Embedding},
year = {2023},
isbn = {9781450399876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579051.3579072},
doi = {10.1145/3579051.3579072},
abstract = {When the estimated probabilities do not match the relative frequencies, we say these estimated probabilities are uncalibrated [39], which may cause incorrect decision making, and is particularly undesired in high-stakes tasks [45]. Knowledge Graph embedding models are reported to produce uncalibrated probabilities [36], e.g., for all the triples predicted with probability 0.9, the percentage of them being truly correct triples is not . In this article, we take a closer look at this problem. First, we confirmed the issue that typical KG Embedding models are uncalibrated. Then, we show how off-the-shelf calibration techniques can be used to mitigate this issue, among which binning-based calibration produces more calibrated probabilities. We also investigated the possible reasons for the uncalibrated probabilities and found that the expit transform, the way used to convert embedding scores into probabilities, is ineffective in most cases.},
booktitle = {Proceedings of the 11th International Joint Conference on Knowledge Graphs},
pages = {104–109},
numpages = {6},
keywords = {Knowledge Graph Embedding, Probability Calibration},
location = {Hangzhou, China},
series = {IJCKG '22}
}

@article{tabacof2019probability,
  title={Probability calibration for knowledge graph embedding models},
  author={Tabacof, Pedro and Costabello, Luca},
  journal={arXiv preprint arXiv:1912.10000},
  year={2019}
}

@article{gawlikowski2023survey,
  title={A survey of uncertainty in deep neural networks},
  author={Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and others},
  journal={Artificial Intelligence Review},
  volume={56},
  number={Suppl 1},
  pages={1513--1589},
  year={2023},
  publisher={Springer}
}

@article{platt1999probabilistic,
  title={Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods},
  author={Platt, John and others},
  journal={Advances in large margin classifiers},
  volume={10},
  number={3},
  pages={61--74},
  year={1999},
  publisher={Cambridge, MA}
}

@misc{joy2022sampledependentadaptivetemperaturescaling,
      title={Sample-dependent Adaptive Temperature Scaling for Improved Calibration}, 
      author={Tom Joy and Francesco Pinto and Ser-Nam Lim and Philip H. S. Torr and Puneet K. Dokania},
      year={2022},
      eprint={2207.06211},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2207.06211}, 
}

@inproceedings{zadrozny2002transformingclassifierscores,
author = {Zadrozny, Bianca and Elkan, Charles},
title = {Transforming classifier scores into accurate multiclass probability estimates},
year = {2002},
isbn = {158113567X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775047.775151},
doi = {10.1145/775047.775151},
abstract = {Class membership probability estimates are important for many applications of data mining in which classification outputs are combined with other sources of information for decision-making, such as example-dependent misclassification costs, the outputs of other classifiers, or domain knowledge. Previous calibration methods apply only to two-class problems. Here, we show how to obtain accurate probability estimates for multiclass problems by combining calibrated binary probability estimates. We also propose a new method for obtaining calibrated two-class probability estimates that can be applied to any classifier that produces a ranking of examples. Using naive Bayes and support vector machine classifiers, we give experimental results from a variety of two-class and multiclass domains, including direct marketing, text categorization and digit recognition.},
booktitle = {Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {694–699},
numpages = {6},
location = {Edmonton, Alberta, Canada},
series = {KDD '02}
}

@inproceedings{zadrozny2001obtaining,
  title={Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers},
  author={Zadrozny, Bianca and Elkan, Charles},
  booktitle={Icml},
  volume={1},
  number={05},
  year={2001}
}


@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@inproceedings{Fey/Lenssen/2019,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}

@inproceedings{Fey/etal/2025,
  title={{PyG} 2.0: Scalable Learning on Real World Graphs},
  author={Fey, Matthias and Sunil, Jinu and Nitta, Akihiro and Puri, Rishi and Shah, Manan and Stojanovi{\v{c}}, Bla{\v{z}} and Bendias, Ramona and Alexandria, Barghi and Kocijan, Vid and Zhang, Zecheng and He, Xinwei and Lenssen, Jan E. and Leskovec, Jure},
  booktitle={Temporal Graph Learning Workshop @ KDD},
  year={2025}
}

@inproceedings{memariani2025linkpredictionnontargeted,
author = {Memariani, Adel and R\"{o}der, Michael and Sharma, Arnab and Demir, Caglar and Ngomo, Axel-Cyrille Ngonga},
title = {Link Prediction Under Non-targeted Attacks: Do Soft Labels Always Help?},
year = {2025},
isbn = {978-3-032-09526-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-09527-5_6},
doi = {10.1007/978-3-032-09527-5_6},
abstract = {Knowledge Graph Embedding (KGE) models rely on precise factual information to learn effective representations. These learned representations support many downstream tasks, with link prediction being a primary application. However, recent studies have shown that noise in training data can compromise the effectiveness of knowledge graph embeddings. Therefore, KGEs are highly vulnerable to data poisoning attacks. Typically current attacks on KGEs are studied under targeted scenarios, where target facts and the model are assumed to be known beforehand. Yet, this information is not often available in real-world scenarios. Thus, more realistic scenarios involve non-targeted but malicious perturbations aimed at reducing the overall model performance. In this paper, we focus on enhancing the robustness of link prediction approaches in non-targeted settings. To mitigate the harmful impact of the noisy data, we explore soft-label loss functions as a strategy for reducing overconfidence in model predictions. We performed a thorough evaluation on six state-of-the-art models and five benchmark datasets, with different noise ratios introduced into each dataset. Our results show that soft labels commonly improve the robustness of KGE models across various noise ratios.},
booktitle = {The Semantic Web – ISWC 2025: 24th International Semantic Web Conference, Nara, Japan, November 2–6, 2025, Proceedings, Part I},
pages = {99–121},
numpages = {23},
keywords = {Knowledge Graph Embeddings, Label Smoothing, Label Relaxation, Non-Adversarial Attacks, Data Poisoning},
location = {Nara, Japan}
}


@inproceedings{naeini2015obtaining,
  title={Obtaining well calibrated probabilities using bayesian binning},
  author={Naeini, Mahdi Pakdaman and Cooper, Gregory and Hauskrecht, Milos},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={29},
  number={1},
  year={2015}
}

@article{glenn1950verification,
  title={Verification of forecasts expressed in terms of probability},
  author={Glenn, W Brier and others},
  journal={Monthly weather review},
  volume={78},
  number={1},
  pages={1--3},
  year={1950},
  publisher={War Department, Office of the Chief Signal Officer}
}

@inproceedings{nixon2019measuring,
  title={Measuring calibration in deep learning.},
  author={Nixon, Jeremy and Dusenberry, Michael W and Zhang, Linchuan and Jerfel, Ghassen and Tran, Dustin},
  booktitle={CVPR workshops},
  volume={2},
  number={7},
  year={2019}
}