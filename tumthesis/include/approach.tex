\chapter{Approach}
\label{chap:approach}

In this chapter, We provide overview of our architecture and techniques used in Knowledge graph link prediction. In the first part, we will focus on task of link prediction by used architecture and regularisation techniques. Later in the second part of the chapter we will focus on the uncertainty estimation, by using different ensemble techniques, and calibration, by using different calibration techniques. 


\section{Model Architecture}
\label{sec:model_architecture}

Our model is inspired from Graph Convolutional Networks architecture proposed in (Kipf et al. 2017) \cite{kipf2017semisupervisedclassificationgraphconvolutional}. We implemented extension of GCN called Relational Graph Convolutional Networks (R-GCN) \cite{schlichtkrull2017modelingrelationaldatagraph}. To understand the architecture we need to understand underlying component of GCNs. Let's begin with \text{Message Passing}.

\subsection{Message Passing}
\label{subsec:message_passing}
 
The nature of our dataset is connectivity between entities which forms a network. To leverage our network structure we need to share information between connected nodes. Idea of the message passing comes from this basic intuition. Each node in the graph must be aware about other nodes in the network. As shown in the figure \ref{fig:message_passing} Message passing consists of Three Stages - \textit{1. Message Receiving,} \textit{2. Message Aggregation} and \textit{3. Message Update}. In the first stage, each node receives messages from its connected immediate neighbours. For node \begin{math} u \end{math}, message from each neighbour node \begin{math}v \in N(u)\end{math}. Here message means embedding of the node. As there might be too many neighbours nodes, in the second stage messages are aggregated using a function \begin{math}AGGREGATE\end{math} like sum, mean or max. In the final stage, aggregated message is used to update the current node embedding of the receiving node by passing function \begin{math}UPDATE\end{math}. This can be mathematically represented as follows:
\begin{align}
    h_{u}^{(k+1)} &= UPDATE^{(k)} \left( h_{u}^{(k)}, AGGREGATE^{(k)} \left( \{ h_{v}^{(k)}, \forall v \in N(u) \} \right) \right) \label{eq:message_passing} \\
                  &= UPDATE^{(k)} \left( h_{u}^{(k)}, m_{u}^{(k+1)} \right) \notag
\end{align}

Where \begin{math}h_{u}^{(k)}\end{math} is the embedding of node \begin{math}u\end{math} at layer \begin{math}k\end{math} and \begin{math}m_{u}^{(k+1)}\end{math} is the aggregated message for node \begin{math}u\end{math} at layer \begin{math}k+1\end{math}. 
\begin{math}AGGREGATE \end{math} function take input embedding of all neighbour nodes 
    \begin{math}v \in N(u)\end{math} at layer \begin{math}k\end{math} and aggregates them to form messaage \begin{math}m_{u}^{(k+1)}
\end{math}. 
\begin{math}
    UPDATE\end{math} function takes \begin{math}m_{u}^{(k+1)} \end{math} and combines it with previous node embedding \begin{math} h_{u}^{k-1}\end{math} to produce new node embedding \begin{math} h_{u}^{(k+1)} \end{math}.
This process happens for all nodes in the graph simultaneously. One iteration of message passing is called a \textit{Message Passing layer}. However, as we know one layer only allows nodes to be aware about their immediate neighbours. This can be extended to multiple layers to allow more distant nodes to share information in one forward pass. First layer where \begin{math} k=0\end{math} is called input layer. Here initial node embeddings can be used. In our case we use one hot encoding as starting information. Final layer \textit{K} is called output layer. Here the embeddings produced can be used for downstream tasks like node classification, link prediction etc. The layers \begin{math} k=1,2,...,K-1\end{math} are called hidden layers. 


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/messagepassing}
    \caption{Figure representing Message Passing}
    \label{fig:message_passing}
\end{figure}

\subsection{Graph Convolutional Network}
\label{subsec:graph_convolutional_network}

Grpah Convolutional Networks uses message passing framework to learn node embeddings. In GCN, the \begin{math}AGGREGATE\end{math} function is defined as follows:
\begin{align}
    m_{u}^{(k+1)} = \sum_{v \in N(u)} \frac{1}{\sqrt{|N(u)|} \sqrt{|N(v)|}} h_{v}^{(k)} \label{eq:gcn_aggregate}
\end{align} Here the messages are normalized by the degree of the nodes to avoid numerical instabilities. and
\begin{equation}
      UPDATE^{(k)}(h,m) = \sigma ( W^{(k)} \cdot  m_{u}^{(k+1)} ) 
\end{equation}  \cite{kipf2017semisupervisedclassificationgraphconvolutional}. Here \begin{math}W^{(k)}\end{math} is the learnable weight matrix at layer \begin{math}k\end{math}.
So our function \ref{eq:message_passing} becomes:
\begin{align}
    h_{u}^{(k+1)} = \sigma \left( W^{(k)} \cdot \sum_{v \in N(u)} \frac{1}{\sqrt{|N(u)|} \sqrt{|N(v)|}} h_{v}^{(k)} \right) \label{eq:gcn_message_passing}
\end{align} Until now we discussed GCN which works pretty well on simple graph which have only one type of relation between nodes. However, We are interested in multi-relational graphs where we have multiple types of relations between nodes. This architecture clearly cannot capture multiple relations information as we are sharing single weight matrix \begin{math}W^{(k)}\end{math} for all relations. Now let's see how R-GCN extends GCN to handle multi-relational graphs.

\subsection{Relational Graph Convolutional Network}
\label{subsec:relational_graph_convolutional_network}
Relational Graph Convolutional Network extends GCN to handle multi-relational graphs by using different weight matrices for different relations. So for each relation \begin{math}r \in R\end{math} we have a separate weight matrix \begin{math}W_{r}^{(k)}\end{math} at layer \begin{math}k\end{math}. The message passing function becomes:
\begin{align}
    h_{u}^{(k+1)} = \sigma \left( \sum_{r \in R} \sum_{v \in N_{r}(u)} \frac{1}{|N_{r}(u)|} W_{r}^{(k)} h_{v}^{(k)} + W_{0}^{(k)} h_{u}^{(k)} \right) \label{eq:rgcn_message_passing}
\end{align} Here \begin{math}N_{r}(u)\end{math} is the set of neighbour nodes of \begin{math}u\end{math} under relation \begin{math}r\end{math} and \begin{math}W_{0}^{(k)}\end{math} is the weight matrix for self loop. This allows each relation to have its own transformation which helps in capturing the multi-relational information in the graph. By this suite of weight matrices, R-GCN can learn more about importance of each relation seperately. However, in dataset like FB15k-237 we have large number of relations which leads to large number of parameter in the model. To overcome this issue, R-GCN uses 2 techniques - \textit{Basis Decomposition} and \textit{Block Diagonal Decomposition}. In Basis Decomposition, We share a number of basis matrices \begin{math}V_{b}^{(k)}\end{math} where \begin{math}b=1,2,...,B\end{math} and each relation weight matrix is represented as a linear combination of these basis matrices. So we have:
\begin{align}
    W_{r}^{(k)} = \sum_{b=1}^{B} a_{rb}^{(k)} V_{b}^{(k)} \label{eq:basis_decomposition}
\end{align} Here \begin{math}a_{rb}^{(k)}\end{math} are the coefficients for relation \begin{math}r\end{math} and basis \begin{math}b\end{math} at layer \begin{math}k\end{math}. This significantly reduces the number of parameters in the model. In Block Diagonal Decomposition, each relation weight matrix is represented as a block diagonal matrix where each block is a smaller matrix.
\begin{align}
    W_{r}^{(k)} = \begin{bmatrix}
    B_{r1}^{(k)} & 0 & \cdots & 0 \\
    0 & B_{r2}^{(k)} & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & B_{rM}^{(k)}
    \end{bmatrix}
 \label{eq:block_diagonal_decomposition}
\end{align}
This also helps in reducing the number of parameters while still allowing each relation to have its own transformation. In our implementation we used Basis Decomposition technique. We will use implementation of R-GCN from PyTorch Geometric library \cite{Fey/etal/2025} \cite{Fey/Lenssen/2019}.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/autoencoder}
    \caption{Figure representing Autoencoder Architecture}
    \label{fig:autoencoder_architecture}
\end{figure}

\subsection{Autoencoder}
\label{subsec:autoencoder}
In the paper \cite{schlichtkrull2017modelingrelationaldatagraph}, R-GCN is used for mainly for two tasks - \textbf{Node Classification} and \textbf{Link Prediction}. In node classification task, the output node embeddings from R-GCN are directly passed to softmax layer for classification. However, in link prediction task, We need to predict missing edges in the graph which requires a scoring function to determine the plausiblity of the triple. To achieve this we use an autoencoder architecture as shown in figure \ref{fig:autoencoder_architecture}. Here as discussed in subsection \ref{subsec:relational_graph_convolutional_network}, R-GCN acts as an encoder which takes the graph as input and produces node embeddings as output. The encoder maps each enitity \begin{math} e \in \mathbf{E}\end{math} to a node embedding \begin{math}h_{e} \in \mathbb{R}^{d}\ \end{math}. Now we want a function SCORING FUNCTION \begin{math} s: (\mathbb{R}^{d} * \mathbf{R} * \mathbb{R}^{d}) \end{math} These node embeddings are then passed to a decoder whose job is to score the liklihood of a triple. As in the paper, DistMult \cite{yang2014embedding} is used as a decoder scoring function. In DistMult, each relation is represented as a diagonal matrix \begin{math}R_{r}\end{math} and the score for a triple \begin{math}(e_h, r, e_t)\end{math} is computed as:
\begin{align}
    f(e_h, r, e_t) = h_{e_h}^{T} R_{r} h_{e_t} \label{eq:distmult_scoring}
\end{align} Here \begin{math}h_{e_h}\end{math} and \begin{math}h_{e_t}\end{math} are the embeddings of head entity \begin{math}e_h\end{math} and tail entity \begin{math}e_t\end{math} respectively. \begin{math} R_{r} \in \mathbb{R}^{d \times d}\end{math} is the diagonal matrix for relation \begin{math} r
\end{math}. The score \begin{math}f(e_h, r, e_t)\end{math} indicates the plausibility of the triple. Higher the score, more likely the triple is valid.

\section{Link Prediction}
\label{sec:link_prediction} 

A graph \begin{math}G=(E,R,T)\end{math} consist of a set of entities \begin{math}E=\{e_1,e_2,...,e_n\}\end{math}, a set of relations \begin{math}R=\{r_1,r_2,...,r_m\}\end{math} and a set of triples \begin{math}T=\{(e_h,r,e_t)|e_h,e_t \in E, r \in R\}\end{math} where \begin{math}e_h\end{math} is head entity, \begin{math}e_t\end{math} is tail entity and \begin{math}r\end{math} is relation between head and tail entity. The task of link prediction is to predict missing triples in the knowledge graph. So we will predict a triple \begin{math}t \notin G\end{math}. We want \begin{math}\operatorname*{arg\,max}_{e_t} P(e_t \mid e_h, r)\end{math} or \begin{math}\operatorname*{arg\,max}_{e_h} P(e_h \mid r, e_t)\end{math}. The score for triple \begin{math}(e_h, r, e_t)\end{math} is computed using scoring function defined in equation \ref{eq:distmult_scoring}.

\subsection{Training}
\label{subsec:training}

As described in the previous works (Bordes et al. 2013)\cite{bordes_translating_2013}, We trained our model using negative sampling. For each positive triple \begin{math}(e_h, r, e_t) \in T\end{math}, we generate \begin{math} \omega \end{math} negative triples by corrupting either head or tail entity. So we have negative triples \begin{math}(e_h', r, e_t)\end{math} or \begin{math}(e_h, r, e_t')\end{math} where \begin{math}e_h' \in E\end{math} and \begin{math}e_t' \in E\end{math}. We generate true labels \begin{math} y \in \{0,1\}\end{math} by assigning 1 for positive triples and 0 for corrupted negative triples. We optimised the model using cross entropy loss over positive and negative triples. The loss function can be defined as:
\begin{align}
    \mathcal{L} = - \sum_{(e_h, r, e_t) \in T} \log \sigma(f(e_h, r, e_t)) - \sum_{(e_h', r, e_t') \in T'} \log (1 - \sigma(f(e_h', r, e_t'))) \label{eq:loss_function}
\end{align} Here \begin{math}T'\end{math} is the set of negative triples generated using negative sampling and \begin{math}\sigma\end{math} is the sigmoid function. We are using negative sampling to train model to distinguish between valid and invalid triples in the knowledge graph. Figure \ref{fig:pipeline} shows the complete pipeline of our approach for link prediction task. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/pipeline}
    \caption{Figure representing Autoencoder Architecture}
    \label{fig:pipeline}
\end{figure}

\subsection{Regularisation Techniques}
\label{subsec:regularisation_techniques}

Issue of overfitting and oversmoothing is prevalent in GCNs \cite{rong2019dropedge}. Overfitting occurs when the model learns to fit the training data too closely and fails to generalise on unseen data. Oversmoothing occurs when node embeddings becomes too similar to each other and converges to a stationary point when we increase number of layers in GCN. We are also interested in the effect of this techniques on uncertainty estimation and calibration. We will used following techniques and evaluate both performance and uncertainty estimation. 


\subsubsection{Edge Dropout}
\label{subsubsec:edge_dropout}
In the earlier work by (Rong et al. 2019)\cite{rong2019dropedge}, DropEdge technique is proposed to tackle both overfitting and oversmoothing in GCNs. DropEdge involves dropping edges during message passing with certain probability p. So during each training iteration, from our message passing set \begin{math}\mathbf{A}\end{math} we randomly choose edges with probabilities \begin{math}p\end{math} to form set \begin{math}\mathbf{A_{drop}}\end{math}. Then we use \begin{math}\mathbf{A'} = \mathbf{A} - \mathbf{A_{drop}}\end{math} for message passing in equation \ref{eq:rgcn_message_passing}. And we use \begin{math}\mathbf{A}\end{math} for negative sampling and loss computation. This way we are forcing the model to learn absence of the edges during message passing and thus improving geenralisation. 
\subsubsection{Label Smoothing}
\label{subsubsec:label_smoothing}
Having a hard ground truths \begin{math} y \end{math} like 0s and 1s for positive and negative samples can lead to overfitting when using cross entropy loss function\cite{szegedy2016rethinking}. To over come this issue, we can use Label Smoothing Regularisation technique proposed by (Szegedy et al. 2016)\cite{szegedy2016rethinking}. In this technique, we soften the hard label by \begin{math} \epsilon \end{math}. So instead of using labels \begin{math} y \in \{0,1\}\end{math}, we use smoothed labels \begin{math} y_{smooth} \end{math} defined as:
\begin{align}
    y_{smooth} = y \cdot (1 - \epsilon) + \frac{\epsilon}{K} \label{eq:label_smoothing}
\end{align} Here \begin{math}K\end{math} is the number of classes. In our case of link prediction, we have binary classification problem so \begin{math}K=2\end{math}. In our case, we set \begin{math}\epsilon = 0.1\end{math}. So our positive labels becomes 0.95 and negative labels becomes 0.05. This way we are preventing the model from becoming too confident about its predictions and thus improving generalisation.

\subsubsection{Dropout}
\label{subsubsec:dropout}
Another regularisation technique we used is Dropout proposed by (Srivastava et al. 2014)\cite{srivastava2014dropout}. In Dropout, during training, we randomly drop units from neural network with certain probability \begin{math}p\end{math}. This prevents the units from co-adapting too much and thus improving generalisation. In our implementation, we applied dropout after each R-GCN layer with dropout probability \begin{math}p=0.2\end{math}. During inference, we use the full network without dropping any units. This technique has been used in many different neural network architectures and has shown to be effective. In previous works\cite{xiao2021node},Dropout was used in the same way as we are using. It has been shown to improve accuracy in GCN node classification tasks. However, its effect on uncertainty estimation and calibration in GCNs is not well studied. We will evaluate this technique in our experiments.


\subsubsection{Batch Normalisation}
\label{subsubsec:batch_normalisation}
Batch Normalisation is a technique proposed by (Ioffe et al. 2015)\cite{ioffe2015batch} to improve training of deep neural networks. In Batch Normalisation, we normalise the activations of each layer to have zero mean and unit variance. This helps in stabilising the training and allows for higher learning rates. In our implementation, we applied batch normalisation after each R-GCN layer before applying non-linearity. This has been shown to improve accuracy in many different neural network architectures. In the recent work by \cite{luobeyond}, Batch Normalisation was studied in GCNs to observe improvemnt in the performance. However, its effect on uncertainty estoimation and calibration is still not well studied. We will evaluate this technique in our experiments.


\section{Uncertainty}
\label{uncertainty}

In this section, We will discuss techniques for uncertainty estimation and calibration. We will explore two different ensemble techniques for uncertainty estimation - Monte Carlo Dropout and Deep Ensembles. We will also explore different calibration techniques for calibrating uncertainty estimates. We will also discuss implementation details of these techniques in our architecture. 

\subsection{Uncertainty Estimation}
\label{subsec:uncertainty_estimation}

As we have discussed in \ref{chap:introduction}, Knowledge Graph Embeddings models are uncalibrated\cite{zhu2023closerlookprobabilitycalibration} \cite{tabacof2019probability}. To tackle this issue, we will use ensemble techniques from Deep learning literature to estimate uncertainty. We have two different uncertainty estimation method - Bayesian Method and Ensemble Method \cite{gawlikowski2023survey}. For Bayesian Method, we will use Monte Carlo Dropout and for Ensemble method, we will use Deep Ensembles. We will implement these techniques in our R-GCN architcture and evaluate their performance on uncertainty estimation. 

\subsubsection{Monte Carlo Dropout}
\label{subsubsec:monte_carlo_dropout}

Monte Carlo Dropout is a technique proposed by (Gal et al. 2016)\cite{gal2016dropoutbayesianapproximationrepresenting} to estimate uncertainty in the neural network. It is a method where we use dropout during inference to perform multiple stochastic forward passes through the network. This allows us to obtain a distribution of the model's predictions, which we can use to estimate uncertainty. To estimate uncertainty using Monte Carlo Dropout, we train the model with dropout enabled. During inference, We perform M stochastic forward passes through the network with dropout enabled. This gives us M different probability distributions for our triple. \begin{align} p(y \mid ( e_h, r, e_t), D) \approx \frac{1}{M} \sum_{i=1}^{M} p(y \mid ( e_h, r, e_t), w_i)  \label{eq:monte_carlo} \end{align} Here \begin{math} p(y \mid ( e_h, r, e_t), D)\end{math} is Predictive Posterior where \begin{math}D\end{math} is training data and \begin{math} w_i \end{math} are the weights sampled from the dropout distribution. In our implementation, we applied sigmoid to convert the scores obtained from equation \ref{eq:distmult_scoring} to form \begin{math} p(y \mid ( e_h, r, e_t), w_i) \end{math}. We use variance of these M predictions to get uncertainty estimate: \begin{align}
    \text{Uncertainty} &= \frac{1}{M} \sum_{i=1}^{M} \left( p(y \mid ( e_h, r, e_t), w_i) - \hat{y} \right)^2 \label{eq:mc_uncertainty}
\end{align}
% do we need a diagram here?


\subsubsection{Deep Ensembles}
\label{subsubsec:deep_ensembles}

Deep Ensemble is a method proposed by (Lakshminarayanan et al.(2017))\cite{lakshminarayanan2017simplescalablepredictiveuncertainty} for estimating predictive uncertainty in deep learning model. The method uses ensemble approach where \begin{math}M\end{math} models are randomly initialised and trained independently on the same dataset. During inference, We perform inference on all \begin{math}M\end{math} models and aggregate the results to obtain final prediction and uncertainty estimate. The paper suggest that Deep Ensemble provides better calibrated predictions as compared to standard single model inference and Monte Carlo Dropout for other Deep Learning architectures. In our implementation, we train \begin{math}M=5\end{math} R-GCN models to test this approach. We use the same architecture and hyperparameters for all models. During inference, we perform prediction with all \begin{math}M\end{math} models and aggregate the results to obtain final prediction. Similar to \nameref{subsubsec:monte_carlo_dropout}, we will perform following steps to obtain final prediction and uncertainty estimate:
\begin{itemize}
    \item Initialise and train \begin{math}M\end{math} models independently on the same dataset.
    \item During inference, perform prediction with all \begin{math}M\end{math} models to obtain \begin{math}  f_i(e_h, r, e_t) \end{math} for \begin{math}i=1,2,...,M\end{math}.
    \item Take a sigmoid of each score to obtain probability to form \begin{math} p(y \mid ( e_h, r, e_t), w_i) \end{math}.
    \item Aggregate the results to obtain final prediction and uncertainty estimate using equations \ref{eq:monte_carlo} and \ref{eq:mc_uncertainty}.
\end{itemize}


\subsection{Calibration Techniques}
\label{sec:uncertainty_calibration}

In earlier section \nameref{subsec:uncertainty_estimation}, we discussed techniques for uncertainty estimation. However, these techniques are not guaranteed to produce calibrated uncertainty estimates. To tackle the issue of uncalibrated predictions, we will use calibration techniques from Deep Learning literature. We will investigate Post-hoc calibration techniques in which we explore both parameteric and non-parameteric calibration techniques. We will also discuss how we implemented these techniques for our architecture.

\subsection{Parameteric Approaches}
\label{subsec:parametric_approaches}
In the parameteric approaches, Platt Scaling\cite{platt1999probabilistic} based approaches like Temperature Scaling and its extension Input Dependent Temperature Scaling (IDTS) \cite{joy2022sampledependentadaptivetemperaturescaling} are used to calibrate the predictions. These methods involve learning a single or multiple parameters to scale the logits obtained from the model before applying softmax or sigmoid function.


\subsubsection{Scalar Temperature Scaling}
\label{subsubsec:scalar_temperature_scaling}
The simplest extension of the platt scaling is Temperature Scaling \cite{guo2017calibrationmodernneuralnetworks}. In this method, we use a single scalar parameter \begin{math} T > 0 \end{math} to scale the logits obtained from the model. Given the logits \begin{math} s = f(e_h, r, e_t) \end{math} from equation \ref{eq:distmult_scoring}, we scale the logits as:
\begin{align}
    p_{calibrated} = \sigma\left(\frac{s}{T}\right) \label{eq:temperature_scaling}
\end{align} We use validation set to learn the optimal value for parameter \begin{math} T \end{math} by minimising the Negative Log Likelihood (NLL) loss between the calibrated probabilities and true labels. This method is post-hoc and does not requires retraining of the model or optimising any parameter from the model. Once we learned the optimal value of \begin{math} T \end{math}, we can use it to calibrate the predictions on test set. If the value of the \begin{math} T \end{math} is greater than 1, it indicates that the model is overconfident and underconfident when the value is less than 1. 

\subsubsection{Input Dependent Temperature Scaling (IDTS)}
\label{subsubsec:input_dependent_temperature_scaling}
This is inspired version of the Sample Dependent Temperature Scaling proposed by (Joy et al. 2022)\cite{joy2022sampledependentadaptivetemperaturescaling} where instead of using a single scalar parameter \begin{math} T \end{math}, a neural network is used to predict a function \begin{math} T(x) \end{math} which maps input \begin{math} x \end{math} to a temperature value. This allows for more sophisticated calibration where temperature scaling is dependent on the input.  This method can capture more complex miscalibration patterns for Out of Distribution (OOD) samples and assign them higher uncertainty. Our method Input Dependent Temperature Scaling is defined as: Given the logits \begin{math} s = f(e_h, r, e_t) \end{math} from equation \ref{eq:distmult_scoring}, we scale the logits as:
\begin{align}
    P_{calibrated} &= \sigma\left(\frac{s}{T(e_h,r)}\right)\label{eq:input_dependent_temperature_scaling} \\
    T(e_h, r) &= \zeta\left( \mathbf{w}_2^\top \text{ReLU}\left( \mathbf{W}_1 [e_h; r] + \mathbf{b}_1 \right) + b_2 \right) + \epsilon 
\end{align}
Here \begin{math} \zeta \end{math} is the softplus function to ensure that temperature is always positive and \begin{math} \epsilon \end{math} is a small constant to avoid division by zero. And \begin{math}[e_h; r] \end{math} denotes concatenation of head entity embedding and relation embedding. \begin{math} \mathbf{W}_1, \mathbf{b}_1, \mathbf{w}_2, b_2 \end{math} are the learnable parameters of the neural network. Similar to Temperature Scaling, we use validation set to learn the parameters of the neural network by minimising the Negative Log Likelihood (NLL) loss between the calibrated probabilities and true labels.
%We need a diagram here 100%

\subsection{Non-Parametric Approaches}
\label{subsec:non_parametric_approaches}

In the non-parameteric approaches, techniques like Isotonic Regression\cite{zadrozny2002transformingclassifierscores} and Histogram Binning\cite{zadrozny2001obtaining} are widely used to calibrate the predicitons\cite{guo2017calibrationmodernneuralnetworks}. These methods do not involve learning any parameters and instead use non-parametric methods to calibrate the predictions. In our usecase, We will analyse isotonic regression as it is best performing calibration technique in previous works\cite{zhu2023closerlookprobabilitycalibration}.


\subsubsection{Isotonic Regression}
\label{subsubsec:isotonic_regression}

Isotonic Regression is a non-parametric calibration technique proposed by (Zadrozny et al. 2002)\cite{zadrozny2002transformingclassifierscores}. In this method, a monotonic function is learned to map the uncalibrated probabilities to calibrated probabilities. Pool Adjacent Violators (PAV) algorithm is used to learn the isotonic fucntion. Given the uncalibrated probabilities \begin{math} p \end{math} obtained from the model and true labels \begin{math} y \end{math}, we learn a monotonic function \begin{math} g \end{math} such that:
\begin{align}
    p_{calibrated} = g(p) \label{eq:isotonic_regression}
\end{align} The function \begin{math} g \end{math} is learned by minimising the Mean Squared Error (MSE) between the calibrated probabilities and true labels on the validation set. This method is post-hoc and does not require retraining of the model. Once the function \begin{math} g \end{math} is learned, it can be used to calibrate the predictions on the test set. Isotonic Regression is particularly useful when the miscalibration pattern is complex and cannot be captured by simple parametric methods like Temperature Scaling.
In our implementation, we used the logits score from equation \ref{eq:distmult_scoring} learned the isotonic function \begin{math} g \end{math} directly on the logits using PAV algorithm on validation set. We then used the learned function to calibrate the predictions on test set. Here we did not applied sigmoid because isotonic regression can work directly on the logits. We used the implementation from scikit-learn library\cite{scikit-learn} for isotonic regression.

