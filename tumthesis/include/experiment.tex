\chapter{Experiment}
\label{chap:experiment}

In this chapter, we describe the experimental setup used in our study for uncertainty Calibration techniques in Knowledge Graphs Link Prediction. We begin with the dataset description and its characteristics. Next, we outline the model configurations, hardware and software specifications, and training protocols for reproducibility. We then define the evaluation metrics used across the experiments, covering both link prediction performance and uncertainty estimation. Finally, we describe the specific experimental protocols followed to assess the impact of the various methods investigated in this thesis.

\section{Dataset Description}
\label{sec:dataset_description}
In our study, we mainly focus on two widely used benchmark datasets for Knowledge Graph Link Prediction: WN18RR and FB15k-237. These datasets are subsets of larger knowledge graphs and have been curated to address inverse relation test leakage problems present in their predecessor datasets, WN18 and FB15k. They provide widely accepted benchmarks for evaluating link prediction models and their uncertainty estimation capabilities. 



\subsection{Dataset Characteristics}
\label{subsec:dataset_statistics}
\textbf{WN18RR} is derived from WordNet, a lexical database of English, and contains 11 relation types with a total of 93,003 triples. It was designed to mitigate test leakage by removing inverse relations present in WN18. The dataset is split into 86,835 training triples, 3,034 validation triples, and 3,134 test triples. 
\newline
\textbf{FB15k-237} is a subset of Freebase, a large collaborative knowledge graph. It contains 237 relation types and a total of 310,116 triples. The dataset is split into 272,115 training triples, 17,535 validation triples, and 20,466 test triples. FB15k-237 was created to eliminate test leakage by removing inverse relations found in FB15k. 


\subsection{Sparsity Analysis (OPTIONAL)}
\label{subsec:dataset_sparsity_analysis}



\section{Experimental Setup}
\label{sec:experimental_setup}

In this section, we detail the experimental setup used to evaluate the performance of our proposed methods. This includes the model configurations, hardware and software specifications, and training protocols.

\subsection{Model Settings}
\label{subsec:model_settings}

The model architecture used in our experiments is illustrated in Figure~\ref{fig:model}. The key hyperparameters are summarized in Appendix Table~\ref{fig:appendix_hyperparameters}. We employed a relational graph convolutional network (RGCN) as the encoder and DistMult as the decoder. We set the hidden layer size and embedding dimensions to 500, with a dropout rate of 0.2 to prevent overfitting. The number of bases for the RGCN was set to 5. We initialise our embeddings with xavier initialization and parameters with glorot initialization. We use Batch Normalisation and dropout of 0.2 after first convolutional layer for stable training. This is our standard vanilla model configuration for all experiments unless otherwise specified. This configuration will be referred as the \textit{base model} in the subsequent sections.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/model}
    \caption{Model Configuration}
    \label{fig:model}
\end{figure}


\subsection{Hardware and Software Used}
\label{subsec:hardware_and_software_used}
We conducted our experiments on a machine equipped with an NVIDIA H100 GPU, 64GB of RAM, and an Intel Xeon processor. The software environment included Python 3.9.21, PyTorch2.8.0+cu128, and Torch Geometric 2.6.1.

\subsection{Training Protocols}
\label{subsec:training_protocols}

For training, we used a learning rate of 0.01, weight decay of 0.0001, and trained the model for up to 10,000 epochs with early stopping based on validation performance. We use negative sampling ratio of 1 and use hard labels 0 and 1. We use edge dropout of 0. 
We trained our models using the Adam optimizer with full batch training. The training process involved monitoring the training and validation loss to ensure convergence and prevent overfitting. We also track Link Prediction performance and Uncertainty Estimation metrics on the validation set every 100 epochs for WN18RR and every 500 epochs for FB15k-237 due to its larger size. We employed early stopping with a patience of 10 evaluation intervals and a delta of 0.001 to halt training when the validation performance ceased to improve. 

\section{Evaluation Metrics}
\label{sec:evaluation_metrics}
In this section, we describe the evaluation metrics used to assess the performance of our models in terms of link prediction accuracy. And on the other hand, we also describe the metrics used to evaluate the quality of the uncertainty estimates produced by our models. We will cover both aspects in details to provide a comprehensive understanding of the model's capabilities. We categorise the metrics into two main groups: Link Prediction Metrics and Uncertainty Estimation Metrics. 

\subsection{Link Prediction Metrics}
\label{subsec:link_prediction_metrics}
In previous research, the ranks are used as main quantitative measure to evaluate link predictions\cite{bordes_translating_2013,dettmers2018convolutional}. The Head and Relation is used as input and score for each entity as tail is calculated using scoring function. We suppose that if our entity embedding is correctly formed than we will have better predictions for tails. However, It is difficult to used ranks every time for comparative and comprehensive analysis therefore we have different metrics which are calculated using ranks. Each of them have their own qualities and purpose. We will list and discuss them one by one. 

\subsubsection{Mean Rank (MR)}
\label{subsubsec:mean_rank}
As we have different number of entities in different datasets, we are interested in the metric which we can compare across different dataset results. Mean Rank is calculated by taking mean of the ranks attained from each correct tail prediction. \ref{eq:mean_rank} shows how Mean Rank is calculated. This is simple, fast and easy to use metric. When mean rank is lower our model makes generally good prediction as it consistently predicting correct tails and making lesser error. However, the downside for this metric is that it penalise the big outliers. When model fails to predict some entity at lower rank. The average becomes higher and thus we can't analyse how good our model actually is.

\begin{equation}
    MR = \frac{1}{N} \sum_{i=1}^{N} rank_i \label{eq:mean_rank}
\end{equation}


\subsubsection{Mean Reciprocal Rank}
\label{subsubsec:mean_reciprocal_rank}

To solve the problems related to the Mean Rank, Mean Reciprocal Rank is used. It is metric where average of reciprocal of the ranks is used. By taking reciprocal, we minimise the effect of outliers on the final results. It helps use to focus on the how consistent our model is in predicting correct tails at lower ranks. The formula for MRR is shown in \ref{eq:mean_reciprocal_rank}. Higher the MRR better the model is performing. 

\begin{equation}
    MRR = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{rank_i} \label{eq:mean_reciprocal_rank}
\end{equation}


\subsubsection{Hits@K}
\label{subsubsec:hits_at_k}
Following the same idea of focusing on lower ranks, Hits@K is used. It calculates the proportion of the correct tail predictions that fall with top K ranks. It gives us exact measurement to evaluate how well model is performing at specific rank zone. In previous studies, K = 1,3,10 is used to measure the performance of the model. Formula for Hits@K is shown in \ref{eq:hits_at_k}. So when we are interested in how many time our model consistently predicting correct tail at rank 1. We will check Hits@1 score. 

\begin{equation}
    Hits@K = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(rank_i \leq K) \label{eq:hits_at_k}
\end{equation}


\subsection{Uncertainty Estimation Metrics}
\label{subsec:uncertainty_estimation_metrics}

To evaluate how well the output probilities of the model are calibrated with the actual correctness likelihood, we use several metrics from the literature on uncertainty estimation and calibration. Metrics like Expected Calibration Error (ECE)\cite{naeini2015obtaining}, Adaptive Calibration Error (ACE)\cite{nixon2019measuring}, Brier Score\cite{glenn1950verification} and Reliability Diagram are widely used to assess the quality of uncertainty estimates in classification tasks. Testing these metrics in the context of Knowledge Graph Link Prediction is different as compared to traditional classification tasks. We adapt these method to suit our link prediction setting. As in real life, We are interested in the top ranked prediction and its associated confidence score. We adapt the accuracy calculation for link prediction by considering a prediction to be correct if the true tail entity is in the top K predicted tails for a given head and relation. We use K = 1 for our experiments. In this way, we can effectively measure both link prediction performance and uncertainty calibration in single testing pass. This maintains ranking based uncertainty evaluation as compared to other literatures \cite{zhu2023closerlookprobabilitycalibration}. We will describe each metric and how it is adapted for our link prediction setting.


\subsubsection{Expected Calibration Error (ECE)}
\label{subsubsec:expected_calibration_error_ece}

Expected Calibration Error (ECE) measures the discrepancy between predicted confidence and actual accuracy across different confidence bins. In this method prediction probilities are divided into M equally spaced bins. For each bin, accuracy and average confidence are calculated. ECE is then computed as the weighted average of the absolute differences between accuracy and confidence across all bins as shown in \ref{eq:ece}. Lower ECE indicates better calibration of the model's uncertainty estimates. In the equation, \( |B_m| \) is the number of samples in bin \( m \), \( N \) is the total number of samples, \( acc(B_m) \) is the accuracy in bin \( m \), and \( conf(B_m) \) is the average confidence in bin \( m \). 
\newline 
In our implementation, we set M = 10 for ECE calculation. As described earlier, we assign true label if the top predicted tail matches the actual tail for given head and relation else false label. This allows us to compute accuracy and confidence for each bin effectively in the link prediction context. However, ECE can be sensitive to binning strategy due to fixed-width bins. Sometimes there might be single sample in a bin which can lead to high variance in accuracy estimates for that bin. Therefore, we also consider Adaptive Calibration Error (ACE) which uses adaptive binning to address this issue.

\begin{equation}
    ECE = \sum_{m=1}^{M} \frac{|B_m|}{N} | acc(B_m) - conf(B_m) | \label{eq:ece}
\end{equation}


\subsubsection{Adaptive Calibration Error (ACE)}
\label{subsubsec:adaptive_calibration_error_ace}
Adaptive Calibration Error (ACE) is similar to ECE but uses adaptive binning to ensure each bin has an equal number of samples. This solves the problem of high variance in accuracy estimates for bins with few samples. In ACE, the predictions are sorted by confidence and divided into M bins with equal number of samples. The ACE is then computed as the average absolute difference between accuracy and confidence across these adaptive bins as shown in \ref{eq:ace}. Lower ACE indicates better calibration. In the equation, \( M \) is the number of bins, \( acc(B_m) \) is the accuracy in bin \( m \), and \( conf(B_m) \) is the average confidence in bin \( m \).

\begin{equation}
    ACE = \frac{1}{M} \sum_{m=1}^{M} | acc(B_m) - conf(B_m) | \label{eq:ace}
\end{equation}


\subsubsection{Brier Score}
\label{subsubsec:brier_score}

Brier Score is a mean square error metric that measures the accuracy of the predicted probabilities. It is calculated as the average squared difference between the predicted probabilities and the actual outcomes (0 or 1) as shown in \ref{eq:brier_score}. Lower Brier Score indicates better calibrated probabilities. In the equation, \( N \) is the total number of samples, \( f_i \) is the predicted probability for sample \( i \), and \( o_i \) is the actual outcome. 

\begin{equation}
    Brier\ Score = \frac{1}{N} \sum_{i=1}^{N} (f_i - o_i)^2 \label{eq:brier_score}
\end{equation}

\subsubsection{Reliability Diagram}
\label{subsubsec:reliability_diagram}
Reliability Diagram provides a visual representation of the calibration of predicted probabilities. It plots the average predicted confidence against the actual accuracy for different confidence bins. If the model is perfectly calibrated, the points will lie on the diagonal line of x=y. Deviations from this line indicate miscalibration, if points lie below the line, the model is overconfident, and if they lie above, it is underconfident. Reliability diagram complements quantitative metrics like ECE and ACE by providing an expressive visualisation of calibration performance across the confidence spectrum.





\section{Experimental Protocols}
\label{sec:experimental_protocols}
We will now describe the specific experimental protocols followed in our study to evaluate the performance of various models and techniques. Our experiments are designed to systematically assess the impact of different methods on link prediction accuracy and uncertainty estimation. Therefore, we will start with a baseline model and progressively introduce various techniques to observe their implication on performance and uncertainty. For each configuration, we will measure link prediction metrics (MR, MRR, Hits@K) and uncertainty estimation metrics (ECE, ACE, Brier Score). We will analyze the results on both datasets: WN18RR and FB15k-237. In all experiments, we will ensure that the training, validation, and test splits remain consistent to allow for fair comparisons across different methods.


\subsection{ Regularization Impact}
\label{subsec:regularization_impact}

\textbf{Objective:}
In this experiment, we will investigate the effect of \nameref{subsubsec:edge_dropout} and \nameref{subsubsec:label_smoothing} on the model's performance and uncertainty estimation. We will compare each variant against the base model. 
\newline
\textbf{Hypothesis:}
In the previous literature, Edge Dropout\cite{rong2019dropedge} and Label Smoothing\cite{memariani2025linkpredictionnontargeted} have been shown to improve generalization and robustness on other architectures for Knowledge Graphs Link Prediction. We hypothesize that these regularization techniques will also enhance uncertainty estimation in our architecture.
\newline
\textbf{Methodology:}
We conduct a controlled comparison by training the base model with and without each regularization technique. We maintain consistent hyperparameters across all experiments to ensure a fair comparison. We will evaluate the Label Smoothing with smoothing factors of 0.1 and 0.2. For Edge Dropout, we will experiment with dropout rates of 0.1, 0.2, and 0.3. For Hybrid Approach, We will combine both techniques to observe their joint effect. 



\subsection{Uncertainty Estimation Methods}
\label{subsec:uncertainty_estimation_methods}
\textbf{Objective:}
In this experiment, we will investigate the effect of \nameref{subsubsec:monte_carlo_dropout} and \nameref{subsubsec:deep_ensembles} on the model's performance and uncertainty estimation. We will compare each variant against the base model. 
\newline
\textbf{Hypothesis:}
In the literature, Monte Carlo Dropout\cite{gal2016dropoutbayesianapproximationrepresenting} and Deep Ensembles\cite{lakshminarayanan2017simplescalablepredictiveuncertainty} have been shown to improve model generalization and robustness in various deep learning architectures. We hypothesize that these uncertainty estimation techniques will provide better calibrated uncertainty estimates in our architecture.
\newline
\textbf{Methodology:}
We conduct a controlled comparison by training the base model, base model with monte carlo and deep ensembles. We maintain consistent hyperparameters across all experiments similar to previous experiment. We will evaluate Monte Carlo Dropout with 5 inference samples. For Deep Ensembles, we will train 5 independent models and aggregate their predictions to estimate uncertainty.


\subsection{Calibration Methods}
\label{subsec:calibration_methods}
\textbf{Objective:}
To evaluate the effectiveness of post-hoc calibration methods, \nameref{subsubsec:scalar_temperature_scaling}, \nameref{subsubsec:input_dependent_temperature_scaling} and \nameref{subsubsec:isotonic_regression}, on improving the calibration of uncertainty estimates in our link prediction model. 
\newline
\textbf{Hypothesis:}
We hypothesize that applying post-hoc calibration methods will lead to better calibrated uncertainty estimates, as measured by reduced ECE and ACE scores, without significantly compromising link prediction performance.
\newline
\textbf{Methodology:}
We will apply each calibration method to the baseline model and the best model from the \ref{subsec:regularization_impact} experiment. As this is a post-hoc technique, we will not retrain the models but will instead used validation set to learn calibration parameters. For temperature scaling, we will initialise the temperature parameter to 1.0, for input-dependent temperature scaling, we will use a small neural network with configuration as fig. \ref{fig:idts_parameter}. For Isotonic Regression, we will use sklearn's implementation with default parameters. We will then evaluate the calibrated models on the test set, measuring both link prediction and uncertainty estimation metrics to assess the impact of calibration.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/idts_parameter}
    \caption{Input-Dependent Temperature Scaling Network Architecture}
    \label{fig:idts_parameter}
\end{figure}


\subsection{Hybrid Approaches}
\label{subsec:hybrid_approaches}

\textbf{Objective:}
To evaluate the combination of uncertainty estimation methods from \ref{subsec:uncertainty_estimation_methods} and calibration methods from \ref{subsec:calibration_methods} on improving both link prediction performance and uncertainty estimation. We will compare how strengths from both approaches can complement each other.
\newline
\textbf{Hypothesis:}
We hypothesize that combining uncertainty estimation methods with calibration techniques will yield more reliable uncertainty estimates as compared to individual methods alone. The lower ECE and ACE scores are expected, along with maintained or improved link prediction metrics.
\newline
\textbf{Methodology:}
We will use the model from \ref{subsec:uncertainty_estimation_methods} and apply the calibration methods from \ref{subsec:calibration_methods}. We will evaluate MonteCarlo Dropout + Temperature Scaling, MonteCarlo Dropout + Input-Dependent Temperature Scaling and Monte Carlo Dropout + Isotonic Regression \& Deep Ensembles + Temperature Scaling, Deep Ensembles + Input-Dependent Temperature Scaling and Deep Ensembles + Isotonic Regression. We will assess the performance of these hybrid models on the test set using the same metrics as before to determine the effectiveness of combining these techniques.