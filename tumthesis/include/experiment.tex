\chapter{Experiment}
\label{chap:experiment}

In this chapter, we describe the experimental setup used in our study for uncertainty Calibration techniques in Knowledge Graphs Link Prediction. We begin with the dataset description and its characteristics. Next, we outline the model configurations, hardware and software specifications, and training protocols for reproducibility. We then define the evaluation metrics used across the experiments, covering both link prediction performance and uncertainty estimation. Finally, we describe the specific experimental protocols followed to assess the impact of the various methods investigated in this thesis.

\section{Dataset Description}
\label{sec:dataset_description}
In our study, we mainly focus on two widely used benchmark datasets for Knowledge Graph Link Prediction: WN18RR and FB15k-237. These datasets are subsets of larger knowledge graphs and have been curated to address inverse relation test leakage problems present in their predecessor datasets, WN18 and FB15k. They provide widely accepted benchmarks for evaluating link prediction models and their uncertainty estimation capabilities. 



\subsection{Dataset Characteristics}
\label{subsec:dataset_statistics}
\textbf{WN18RR} is derived from WordNet, a lexical database of English, and contains 11 relation types with a total of 93,003 triples. It was designed to mitigate test leakage by removing inverse relations present in WN18. The dataset is split into 86,835 training triples, 3,034 validation triples, and 3,134 test triples. 
\newline
\textbf{FB15k-237} is a subset of Freebase, a large collaborative knowledge graph. It contains 237 relation types and a total of 310,116 triples. The dataset is split into 272,115 training triples, 17,535 validation triples, and 20,466 test triples. FB15k-237 was created to eliminate test leakage by removing inverse relations found in FB15k. 


\subsection{Sparsity Analysis (OPTIONAL)}
\label{subsec:dataset_sparsity_analysis}



\section{Experimental Setup}
\label{sec:experimental_setup}

In this section, we detail the experimental setup used to evaluate the performance of our proposed methods. This includes the model configurations, hardware and software specifications, and training protocols.

\subsection{Model Settings}
\label{subsec:model_settings}

The model architecture used in our experiments is illustrated in Figure~\ref{fig:model}. The key hyperparameters are summarized in Appendix Table~\ref{fig:appendix_hyperparameters}. We employed a relational graph convolutional network (RGCN) as the encoder and DistMult as the decoder. We set the hidden layer size and embedding dimensions to 500, with a dropout rate of 0.2 to prevent overfitting. The number of bases for the RGCN was set to 5. We initialise our embeddings with xavier initialization and parameters with glorot initialization. We use Batch Normalisation and dropout of 0.2 after first convolutional layer for stable training. This is our standard vanilla model configuration for all experiments unless otherwise specified. This configuration will be referred as the \textit{base model} in the subsequent sections.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/model}
    \caption{Model Configuration}
    \label{fig:model}
\end{figure}


\subsection{Hardware and Software Used}
\label{subsec:hardware_and_software_used}
We conducted our experiments on a machine equipped with an NVIDIA H100 GPU, 64GB of RAM, and an Intel Xeon processor. The software environment included Python 3.9.21, PyTorch2.8.0+cu128, and Torch Geometric 2.6.1.

\subsection{Training Protocols}
\label{subsec:training_protocols}

For training, we used a learning rate of 0.01, weight decay of 0.0001, and trained the model for up to 10,000 epochs with early stopping based on validation performance. We use negative sampling ratio of 1 and use hard labels 0 and 1. We use edge dropout of 0. 
We trained our models using the Adam optimizer with full batch training. The training process involved monitoring the training and validation loss to ensure convergence and prevent overfitting. We also track Link Prediction performance and Uncertainty Estimation metrics on the validation set every 100 epochs for WN18RR and every 500 epochs for FB15k-237 due to its larger size. We employed early stopping with a patience of 10 evaluation intervals and a delta of 0.001 to halt training when the validation performance ceased to improve. 

\section{Evaluation Metrics}
\label{sec:evaluation_metrics}
In this section, we describe the evaluation metrics used to assess the performance of our models in terms of link prediction accuracy. And on the other hand, we also describe the metrics used to evaluate the quality of the uncertainty estimates produced by our models. We will cover both aspects in details to provide a comprehensive understanding of the model's capabilities. We categorise the metrics into two main groups: Link Prediction Metrics and Uncertainty Estimation Metrics. 

\subsection{Link Prediction Metrics}
\label{subsec:link_prediction_metrics}


\subsubsection{Mean Rank}
\label{subsubsec:mean_rank}

\subsubsection{Mean Reciprocal Rank}
\label{subsubsec:mean_reciprocal_rank}
\subsubsection{Hits@K}
\label{subsubsec:hits_at_k}


\subsection{Uncertainty Estimation Metrics}
\label{subsec:uncertainty_estimation_metrics}

\subsubsection{Expected Calibration Error (ECE)}
\label{subsubsec:expected_calibration_error_ece}
\subsubsection{Adaptive Calibration Error (ACE)}
\label{subsubsec:adaptive_calibration_error_ace}

\subsubsection{Brier Score}
\label{subsubsec:brier_score}

\subsubsection{Reliability Diagram}
\label{subsubsec:reliability_diagram}


\section{Experimental Protocols}
\label{sec:experimental_protocols}
We will now describe the specific experimental protocols followed in our study to evaluate the performance of various models and techniques. Our experiments are designed to systematically assess the impact of different methods on link prediction accuracy and uncertainty estimation. Therefore, we will start with a baseline model and progressively introduce various techniques to observe their implication on performance and uncertainty. For each configuration, we will measure link prediction metrics (MR, MRR, Hits@K) and uncertainty estimation metrics (ECE, ACE, Brier Score). We will analyze the results on both datasets: WN18RR and FB15k-237. In all experiments, we will ensure that the training, validation, and test splits remain consistent to allow for fair comparisons across different methods.


\subsection{ Regularization Impact}
\label{subsec:regularization_impact}

\textbf{Objective:}
In this experiment, we will investigate the effect of \nameref{subsubsec:edge_dropout} and \nameref{subsubsec:label_smoothing} on the model's performance and uncertainty estimation. We will compare each variant against the base model. 
\newline
\textbf{Hypothesis:}
In the previous literature, Edge Dropout\cite{rong2019dropedge} and Label Smoothing\cite{memariani2025linkpredictionnontargeted} have been shown to improve generalization and robustness on other architectures for Knowledge Graphs Link Prediction. We hypothesize that these regularization techniques will also enhance uncertainty estimation in our architecture.
\newline
\textbf{Methodology:}
We conduct a controlled comparison by training the base model with and without each regularization technique. We maintain consistent hyperparameters across all experiments to ensure a fair comparison. We will evaluate the Label Smoothing with smoothing factors of 0.1 and 0.2. For Edge Dropout, we will experiment with dropout rates of 0.1, 0.2, and 0.3. For Hybrid Approach, We will combine both techniques to observe their joint effect. 



\subsection{Uncertainty Estimation Methods}
\label{subsec:uncertainty_estimation_methods}
\textbf{Objective:}
In this experiment, we will investigate the effect of \nameref{subsubsec:monte_carlo_dropout} and \nameref{subsubsec:deep_ensembles} on the model's performance and uncertainty estimation. We will compare each variant against the base model. 
\newline
\textbf{Hypothesis:}
In the literature, Monte Carlo Dropout\cite{gal2016dropoutbayesianapproximationrepresenting} and Deep Ensembles\cite{lakshminarayanan2017simplescalablepredictiveuncertainty} have been shown to improve model generalization and robustness in various deep learning architectures. We hypothesize that these uncertainty estimation techniques will provide better calibrated uncertainty estimates in our architecture.
\newline
\textbf{Methodology:}
We conduct a controlled comparison by training the base model, base model with monte carlo and deep ensembles. We maintain consistent hyperparameters across all experiments similar to previous experiment. We will evaluate Monte Carlo Dropout with 5 inference samples. For Deep Ensembles, we will train 5 independent models and aggregate their predictions to estimate uncertainty.


\subsection{Calibration Methods}
\label{subsec:calibration_methods}
\textbf{Objective:}
To evaluate the effectiveness of post-hoc calibration methods, \nameref{subsubsec:scalar_temperature_scaling}, \nameref{subsubsec:input_dependent_temperature_scaling} and \nameref{subsubsec:isotonic_regression}, on improving the calibration of uncertainty estimates in our link prediction model. 
\newline
\textbf{Hypothesis:}
We hypothesize that applying post-hoc calibration methods will lead to better calibrated uncertainty estimates, as measured by reduced ECE and ACE scores, without significantly compromising link prediction performance.
\newline
\textbf{Methodology:}
We will apply each calibration method to the baseline model and the best model from the \ref{subsec:regularization_impact} experiment. As this is a post-hoc technique, we will not retrain the models but will instead used validation set to learn calibration parameters. For temperature scaling, we will initialise the temperature parameter to 1.0, for input-dependent temperature scaling, we will use a small neural network with configuration as fig. \ref{fig:idts_parameter}. For Isotonic Regression, we will use sklearn's implementation with default parameters. We will then evaluate the calibrated models on the test set, measuring both link prediction and uncertainty estimation metrics to assess the impact of calibration.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/idts_parameter}
    \caption{Input-Dependent Temperature Scaling Network Architecture}
    \label{fig:idts_parameter}
\end{figure}


\subsection{Hybrid Approaches}
\label{subsec:hybrid_approaches}

\textbf{Objective:}
To evaluate the combination of uncertainty estimation methods from \ref{subsec:uncertainty_estimation_methods} and calibration methods from \ref{subsec:calibration_methods} on improving both link prediction performance and uncertainty estimation. We will compare how strengths from both approaches can complement each other.
\newline
\textbf{Hypothesis:}
We hypothesize that combining uncertainty estimation methods with calibration techniques will yield more reliable uncertainty estimates as compared to individual methods alone. The lower ECE and ACE scores are expected, along with maintained or improved link prediction metrics.
\newline
\textbf{Methodology:}
We will use the model from \ref{subsec:uncertainty_estimation_methods} and apply the calibration methods from \ref{subsec:calibration_methods}. We will evaluate MonteCarlo Dropout + Temperature Scaling, MonteCarlo Dropout + Input-Dependent Temperature Scaling and Monte Carlo Dropout + Isotonic Regression \& Deep Ensembles + Temperature Scaling, Deep Ensembles + Input-Dependent Temperature Scaling and Deep Ensembles + Isotonic Regression. We will assess the performance of these hybrid models on the test set using the same metrics as before to determine the effectiveness of combining these techniques.