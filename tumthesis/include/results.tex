\chapter{Results}
\label{chap:results}

\section{Experiment 1: Regularization Impact}
\label{sec:experiment_1_regularization_impact}
This section presents the results of Experiment 1, which investigates the impact of different regularization techniques on the performance and uncertainty calibration of RGCN model. We compare the baseline model with models employing edge dropout, label smoothing, and a combination of both techniques.


\subsection{Link Prediction Performance}
\label{subsec:link_prediction_performance}

Table \ref{tab:link_prediction_performance} summarizes the performance of various models on the WN18RR and FB15k-237 datasets using standard link prediction metrics: Mean Reciprocal Rank (MRR) and Hits@K (K=1,3,10).

\begin{itemize}
    \item \textbf{Baseline}: The baseline RGCN model is a vanilla version where we have not applied edge dropout or label smoothing. We can see that the baseline model achieves an MRR of 0.396 in WN18RR dataset and 0.222 in FB15k-237 dataset. Hits@1 for both datasets are 0.376 and 0.131 respectively. These results serves as a reference point for evaluating the impace of regularization techniques.
    \item \textbf{Edge Dropout}: Introducing Edge dropout with rates of 0.1 and 0.2 improves the MRR to 0.404 and 0.405 in the WN18RR and to 0.254 and 0.255 in FB15k-237, respectively. The Hits@1 and Hits@3 metrics also show improvement with edge dropout, particularly at the 0.2 rate, which achieves the highest scores across all metrics for both datasets.
    \item \textbf{Label Smoothing}: Applying label smoothing with values of $\omega = 0.1$ and $\omega = 0.2$ yields MRRs of 0.407 and 0.396 for WN18RR, respectively. The Hits@1 metric also shows improvement, particularly with $\omega = 0.1$, reaching 0.386. While, for FB15K-237, the performance was totally opposite as MRR dropped to 0.221 and 0.222 respectively and Hits@1 also dropped to 0.132 for both values of $\omega$. 
    \item \textbf{Combined Techniques}: For WN18RR, The best performance is observed when both edge dropout (0.2) and label smoothing (0.1) are combinely applied, achieving highest MRR of 0.4386 and Hits@1 of 0.40. This combination yields highest scoress across all metrics, indicating a synergistic effect of the two regularization methods. However, for FB15k-237, the combined techniques was better than baseline and label smoothing but not better than edge dropout alone. 
\end{itemize}


\begin{table}[htbp]
        \centering
        \includegraphics[width=\textwidth]{figures/experiment1/exp1table}
        \caption{Link Prediction Performance on WN18RR and FB15k-237 Datasets}
        \label{tab:link_prediction_performance}
\end{table}




\subsection{Uncertainty Metrics Analysis}
\label{subsec:uncertainty_metrics_analysis}

Figure \ref{tab:uncertainty_metrics_analysis} illustrates the impact of different regularization techniques on uncertainty estimation metrics, including Expected Calibration Error (ECE) and Reliability Diagram. Key observations include:
\begin{itemize}
    \item \textbf{Basline} shows the highest ECE among all models, indicating worse calibration compared to regularized models. Both of the models are under confident in their predictions as we can see in reliability diagram \ref{tab:uncertainty_metrics_analysis} where the red line is above the diagonal line for both datasets. For WN18RR, the ECE is 0.318, while for FB15K-237, the ECE is 0.3003. We can also observe that most of the predictions are concentrated in the lower confidence bins near 0. This shows that the RGCN model are not well calibrated and needs calibration techniques to make them reliable in real world applications.

    \item \textbf{Edge Dropout} The ECE score for edge dropout models are not significantly different from the baseline, with values of 0.299 and 0.301 for WN18RR and for FB15K-237 0.271 and 0.254 for edge dropout rates of 0.1 and 0.2, respectively. It is interesting to note that nature of the reliability diagram for edge dropout models is similar to the baseline, with predictions still concentrated in the lower confidence bins.
    

    \item \textbf{Label Smoothing} Label smoothing models show a signicant change in the models behavior as the ECE scores dropped and the reliability diagram shows that the model changed from squeeze the predictions in the lower confidence bins to more spread out across the confidence spectrum. Best performance is observed with $\omega = 0.2$ for both datasets, achieving ECE scores dropping by 19\% for WN18RR and by 36\% for FB15K-237 compared to the baseline. This can be observed in the reliability diagram where the orange line is much closer to the diagonal line compared to other models. And lowest and highest confidence bin are near 0.1 and 0.9 for $\omega = 0.2$ which shows that label smoothing is working as expected.
    
    \item \textbf{Combined Techniques} The combination of edge dropout and label smoothing yields the best calibration performance, with ECE scores of 0.238 for WN18RR and 0.158 for FB15K-237. This is a signicant improvement over the baseline and individual regularization techniques. In the reliability diagram, the yellow line represents the combined techniques and is closest to the diagonal line and the confidence bins are more evenly distributed across the spectrum as compared to the baseline and other regularization techniques. 
\end{itemize}


\begin{table}[htbp]
        \centering
        \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/experiment1/exp1graph2}
        \end{minipage}
        \hfill
        \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/experiment1/exp1graph3}
        \end{minipage}
        \caption{Uncertainty Metrics Analysis on WN18RR and FB15K-237 Datasets with Different Regularization Techniques. Here Edge Dropout is denoted as ED and Label Smoothing as LS.}
        \label{tab:uncertainty_metrics_analysis}
\end{table}


\section{Experiment 2: Uncertainty Estimation Methods}
\label{sec:experiment_2_uncertainty_estimation_methods}

This section presents the results of Experiment 2, which evaluates monte carlo dropout (MC Dropout) and deep ensembles as uncertainty estimation methods applied to the best performing regularized RGCN model from Experiment 1 - For WN18RR, Baseline + ED + LS and For FB15K-237, Baseline + ED. The performance and uncertainty estimation metrics are compared against the baseline regularized model without uncertainty estimation.

\subsection{Link Prediction Performance}
\label{subsec:link_prediction_performance_exp2}
Table \ref{tab:link_prediction_performance_exp2} summarizes the link prediction performance of the uncertainty estimation methods on the WN18RR and FB15k-237 datasets. Key observations include:

\begin{itemize}
        \item \textbf{Monte Carlo Dropout} MC Dropout applied to the best model from Experiment 1 shows slight degrading in link prediction performance compared to the baseline regularized model. For WN18RR, the MRR drops from 0.4386 to 0.4353 and for FB15K-237, the MRR drops from 0.255 to 0.253. This is not a significant drop and can be attributed to the stochastic nature of MC Dropout during inference.  
        \item \textbf{Deep Ensembles} Deep Ensembles shows signicant improvement in link prediction performance compared to the baseline regularized model. For WN18RR, the MRR increases from 0.4386 to 0.456 which is approximately 4\% improvement and for FB15K-237, the MRR increases from 0.255 to 0.286 which is approximately 12\% improvement. This shows leveraging multiple diverse models in an ensemble can capture different aspects of the data and lead to better generalization and performance.
\end{itemize} 

\begin{table}[htbp]
        \centering
        \includegraphics[width=1\textwidth]{figures/experiment2/table}
        \caption{Link prediction performance of Uncertainty Estimation Methods on WN18RR and FB15k-237 Datasets. Here, Left table is for WN18RR and right table is for FB15K-237. The best performing regularized model from Experiment 1 is used as the baseline for comparison. Best performance is highlighted in bold.}
        \label{tab:link_prediction_performance_exp2}
\end{table}


\subsection{Reliability Diagrams}
\label{subsec:reliability_diagrams}


In this section, we analyze the reliability diagrams for the uncertainty estimation methods from the previous section. This section provides insights into how well the predicted probabilities from the models align with the actual outcomes. We will compare the reliability diagrams of the baseline regularized model with those of the Monte Carlo Dropout and Deep Ensemble methods. 

\begin{itemize}
        \item \textbf{Monte Carlo Dropout} Monte Carlo Dropout decreases the ECE score in both datasets. For WN18RR, the ECE score dropped marginally from 0.238 to 0.234 and for FB15K237, It dropped from 0.254 to 0.251. This shows that Monte Carlo Dropout doesn't have much significant effect on the calibration of the model. We have also measured the variance for the positive triples in the test set and negative triples generated by negative sampling. We observed that the variance for the positive triples is 0.0719 and for the negative triples is 0.0558 which shows that model is more struggling to predict the positive triples as compared to the negative triples.

        \item \textbf{Deep Ensemble} We can see similar effect in Deep Ensemble as Monte Carlo Dropout. We can observed marginal improvement in the ECE score as compared to baseline from 0.2383 to 0.233 in WN18RR. However, The Deep Ensemble model have better calibrated probabilities in FB15K237 dataset. We can see signicant drop in ECE score from 0.254 to 0.193.  We observed that the variance for the positive triples is 0.0866 and for the negative triples is 0.1920 which shows opposite behavior as compared to Monte Carlo Dropout. Here, the model is struggling more to predict the negative triples as compared to the positive triples.
\end{itemize}



\begin{table}[htbp]
        \centering
        \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/experiment2/graph_wrr}
        \end{minipage}
        \hfill
        \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/experiment2/graph_fb15}
        \end{minipage}
        \caption{Uncertainty Metrics Analysis on WN18RR and FB15K-237 Datasets with Different Uncertainty Estimation Methods. Here Edge Dropout is denoted as ED and Label Smoothing as LS.}
        \label{tab:uncertainty_metrics_analysis_exp2}
\end{table}


\subsection{Computational Overhead (OPTIONAL)}
\label{subsec:computational_overhead}

\section{Experiment 3: Calibration Methods}
\label{sec:experiment_3_calibration_methods}

In this section, we calibrate the best performing regularized model from \ref{sec:experiment_1_regularization_impact} with different calibration methods and analyze the impact on link prediction performance and uncertainty metrics. We apply Temperature scaling, Platt scaling and Isotonic regression to the baseline regularized model and compare the results with the uncalibrated model. We will understand how these calibration methods affect the model's confidence in its predictions with the help of Figure \ref{tab:compare_metrics_exp3} and \ref{tab:reliability_diagram_exp3}

\begin{table}[htbp]
        \centering
        \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/experiment3/graph_wrr}
        \end{minipage}
        \hfill
        \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/experiment3/graph_fb15k}
        \end{minipage}
        \caption{Uncertainty Metrics Analysis on WN18RR and FB15K-237 Datasets with Different Uncertainty Estimation Methods. Here Edge Dropout is denoted as ED and Label Smoothing as LS.}
        \label{tab:compare_metrics_exp3}
\end{table}

\subsection{Comparision of Calibration Methods}
\label{subsec:effect_on_link_prediction_performance_exp3}
Table \ref{tab:compare_metrics_exp3} shows the link prediction and uncertainty metrics for the different calibration methods applied to the best performing regularized model from \ref{sec:experiment_1_regularization_impact}. For WN18RR, We calibrated the Baseline + ED + LS model and for FB15K-237, we calibrated the Baseline + ED model. The following observations can be made from the results:
\begin{itemize}
        \item \textbf{Temperature scaling} shows improvement in the Brier score and ECE score for both datasets. It also preserves the link prediction performance with no drop in MRR and Hits@K metrics. Though, the reliability diagram in \ref{tab:reliability_diagram_exp3} shows that the model is still under confident in its predictions as the curve is above the diagonal line for both datasets in the higher confidence bins.
        \item \textbf{Platt scaling} also have similar nature in the link prediction score as temperature scaling as no change in the link prediction performance is observed. Furthermore, Platt scaling shows significant improvement in the uncertainty calibration as compared to temperature scaling. The ECE score dropped from 0.238 to 0.031 and Brier score also dropped from 0.397 to 0.108 for WN18RR. For FB15K-237, the ECE score dropped from 0.254 to 0.035 and Brier score also dropped from 0.231 to 0.079. This shows Platt Scaling is better as compared to temperature scaling in all aspects. We can observe in the \ref{tab:reliability_diagram_exp3} that the curve for Platt scaling is much closely aligned to the diagonal line as compared to temperature scaling.
        \item \textbf{Isotonic regression} shows slight degrading performance in the link prediction metrics for both datasets. For both datasets, MRR and Hits@k dropped by approximately 2\%. However, Isotonic regression shows improvement in the calibration metrics as compared to the uncalibrated model. For WN18RR, the ECE score dropped from 0.238 to 0.147 and Brier score also dropped from 0.397 to 0.1427. For FB15K-237, the ECE score dropped from 0.254 to 0.119 and Brier score also dropped from 0.231 to 0.174. This shows that Isotonic regression is better than temperature scaling but not better than Platt scaling in terms of calibration performance. As we can also observe the same in the reliability diagram in \ref{tab:reliability_diagram_exp3} where the curve for Isotonic regression is in between the curve for Platt scaling and Temperature scaling. 
\end{itemize}
 
Overall, Platt scaling is the best performing calibration method among the three methods in terms of both link prediction performance and uncertainty calibration. 
\begin{table}[htbp]
        \centering
        \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/experiment3/graph_rea_wn18}
        \end{minipage}
        \hfill
        \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/experiment3/graph_rea_fb15k}
        \end{minipage}
        \caption{Link prediction performance of Uncertainty Estimation Methods on WN18RR and FB15K-237}
        \label{tab:reliability_diagram_exp3}
\end{table}



\section{Experiment 4: Hybrid Approaches}
\label{sec:experiment_4_hybrid_approaches}

In this section, we explore the best combination of regularization techniques, uncertainty estimation methods and calibration methods to achieve the best out of all worlds scenario. We combine the best performing regularized model from Experiment 1 with the best uncertainty estimation method from Experiment 2 and the best calibration method from Experiment 3. We analyze the link prediction performance and uncertainty metrics of these hybrid approaches.

\begin{table}[htbp]
        \centering
        \includegraphics[width=1\textwidth]{figures/experiment4/table}
        \caption{Link prediction performance of Uncertainty Estimation Methods. Here, We measured ECE score with 10 bins to understand the calibration performance of the models. We list ECE score for the uncalibrated model and calibrated models with Temperature scaling, Platt scaling and Isotonic regression.}
        \label{tab:ece_table_exp4}
\end{table}


\subsection{Quantitative Analysis}
\label{subsec:quantitative_analysis_exp4}
We analyse the numerical performance of the experiment from \ref{tab:ece_table_exp4}.

\begin{itemize}
        \item \textbf{Effect of Calibration} \begin{itemize}
                \item \textit{Starting point} Deep Ensemble models provides better starting point as compared to the Monte Carlo Methods. In WN18RR, the ECE score for uncalibrated Deep Ensemble model is lesser than the ECE score for uncalibrated Monte Carlo Dropout model.
                \item \textit{Platt Scaling} We can observe and confirm that Platt scaling is the best performing calibration method as we have seen in the last experiment. Platt scaling provides the best calibration performance in both uncertainty estimation methods across both datasets. For WN18RR, the ECE score for Platt scaling is 0.071 for Monte Carlo Dropout and 0.01 for Deep Ensemble. For FB15K-237, the ECE score for Platt scaling is 0.031 for Monte Carlo Dropout and 0.024 for Deep Ensemble.
                \item \textit{Failure} As we have observed in the last experiment, Temperature scaling is the worst performing calibration method among the three methods. It showed very slight improvement in the ECE score for both uncertainty estimation methods across both datasets. For WN18RR, around 17\% improvement in ECE score is observed for Monte Carlo Dropout and around 13\% improvement is observed for Deep Ensemble. For FB15K-237, around 16\% improvement in ECE score is observed for Monte Carlo Dropout and around 5\% improvement is observed for Deep Ensemble. 
        \end{itemize}
        \item \textbf{Effect of Link Prediction} \begin{itemize}
                \item \textit{Stablity of Parametric Calibration Methods} Both Temperature scaling and Platt scaling shows stablity in the link prediction performance as no significant drop in the MRR and Hits@K metrics is observed. 
                \item \textit{Failure of Isotonic Regression} Isotonic regression shows degrading performance in the link prediction metrics for both uncertainty estimation methods across both datasets. About 60\% drop in MRR and 87\% drop Hits@K metrics is observed for Monte Carlo Dropout and about 20\% drop in MRR and 37.5\% drop in Hits@K metrics is observed for Deep Ensemble. This suggests a underlying issue with the isotonic regression method when applied to the uncertainty estimation methods in our setting. We will discuss the possible reasons for this failure in the next chapter.
        \end{itemize}
\end{itemize}



\subsection{Qualitative Analysis}
\label{subsec:qualitative_analysis_exp4}
\begin{itemize}
        \item \textbf{Temperature Scaling} The red curve in both reliability diagrams in \ref{fig:compare_metrics_wn_exp4} and \ref{fig:compare_metrics_fb_exp4} represents the temperature scaling method. We can see that the curve is far from the diagonal line for the higher confidence bins which shows that the model is under confident in its predictions. 
        \item \textbf{Platt Scaling} The blue curve in both reliability diagrams represents the Platt scaling method. We can see that the curve is closely aligned to the diagonal line for both datasets which shows that the model is well calibrated in its predictions.
        \item \textbf{Isotonic Regression} The Green curve represents Isotonic regression method. Consistent with our observations in the \ref{sec:experiment_3_calibration_methods}, the curve for isotonic regression is in between the curve for Platt scaling and Temperature scaling. It is closer to the diagonal line as compared to temperature scaling but not as close as Platt scaling. 
\end{itemize}




\begin{figure}[htbp]
        \centering
        \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/experiment4/figure_wn_mc}
        \end{minipage}
        \hfill
        \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/experiment4/figure_wn_en}
        \end{minipage}
        \caption{Reliability Curves on WN18RR Dataset with Different Calibration Methods applied to Monte Carlo Dropout and Deep Ensemble Uncertainty Estimation Methods.}
        \label{fig:compare_metrics_wn_exp4}
\end{figure}


\begin{figure}[htbp]
        \centering
        \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/experiment4/figure_fb_mc}
        \end{minipage}
        \hfill
        \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/experiment4/figure_fb_en}
        \end{minipage}
        \caption{Reliability Curves on FB15K-237 Dataset with Different Calibration Methods applied to Monte Carlo Dropout and Deep Ensemble Uncertainty Estimation Methods.}
        \label{fig:compare_metrics_fb_exp4}
\end{figure}
